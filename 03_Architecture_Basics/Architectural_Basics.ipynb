{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        #Input will be a 28*28*1 image and output of this Sequential block wil be 22*22*32, RF is 3*3\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, bias=False), #After convolving, this will become 26*26\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            #Since we have convoluted once using stride 1, RF has become 5*5\n",
        "            nn.Conv2d(16, 16, 3, bias=False), \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            #RF here will be 7*7\n",
        "            nn.Conv2d(16, 32, 3, bias=False), #After convolving, this will become 24*24\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        #Input will be a 22*22*32 and output of this Sequential block wil be 11*11*16, RF is 7*7\n",
        "        self.trans1 = nn.Sequential(\n",
        "            nn.Conv2d(32, 16, 1, bias=False), \n",
        "            nn.ReLU(),\n",
        "\n",
        "             #RF here will be 14*14\n",
        "            nn.MaxPool2d(2, 2), #After Max Pooling this become 14*14\n",
        "        )\n",
        "        \n",
        "        #Input will be a 11*11*16 and output of this Sequential block wil be 7*7*16, RF is 16*16\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            #RF here will be 18*18\n",
        "            nn.Conv2d(16, 16, 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        #Input will be 7*7*16 and output of this Sequential block wil be 5*5*16, RF is 20*20\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            #RF here will be 22*22\n",
        "            nn.Conv2d(16, 16, 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        # GAP Layer\n",
        "        # input here will be 5*5*16; output - 1*1*10\n",
        "        self.avg_pool = nn.Sequential(\n",
        "            #RF here is 22*22\n",
        "            nn.Conv2d(16, 10, 1, bias=False),\n",
        "            nn.AvgPool2d(5)\n",
        "        )\n",
        "\n",
        "   \n",
        "    def forward(self, x): # Forward function defines the computation at every call( Takes x: input, returns log_softmax(x) as output)\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.avg_pool(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Batch normalization is used after every convolution.\n",
        "* Bias is set as False to remove bias terms.\n",
        "* Dropout of 0.1 is applied on every convolution. It is not applied before last layer since, last layer is our CEO and we cannot hide anything from it. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zrUbLY2QmZfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary #This package provides information complementary to what is provided by print(your_model) in PyTorch\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q33TbL_6mU5z",
        "outputId": "f7a81cd5-d50c-4af8-a2c3-47e58e48b993"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075630bc-a145-4675-d9be-9118be10712e"
      },
      "source": [
        "\n",
        "use_cuda = torch.cuda.is_available() #checking if CUDA is available\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device) # This transfers the model to the device\n",
        "summary(model, input_size=(1, 28, 28))# This prints the model summary"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             144\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "         Dropout2d-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           2,304\n",
            "              ReLU-6           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
            "         Dropout2d-8           [-1, 16, 24, 24]               0\n",
            "            Conv2d-9           [-1, 32, 22, 22]           4,608\n",
            "             ReLU-10           [-1, 32, 22, 22]               0\n",
            "      BatchNorm2d-11           [-1, 32, 22, 22]              64\n",
            "        Dropout2d-12           [-1, 32, 22, 22]               0\n",
            "           Conv2d-13           [-1, 16, 22, 22]             512\n",
            "             ReLU-14           [-1, 16, 22, 22]               0\n",
            "        MaxPool2d-15           [-1, 16, 11, 11]               0\n",
            "           Conv2d-16             [-1, 16, 9, 9]           2,304\n",
            "             ReLU-17             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-18             [-1, 16, 9, 9]              32\n",
            "        Dropout2d-19             [-1, 16, 9, 9]               0\n",
            "           Conv2d-20             [-1, 16, 7, 7]           2,304\n",
            "             ReLU-21             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-22             [-1, 16, 7, 7]              32\n",
            "        Dropout2d-23             [-1, 16, 7, 7]               0\n",
            "           Conv2d-24             [-1, 16, 7, 7]           2,304\n",
            "             ReLU-25             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-26             [-1, 16, 7, 7]              32\n",
            "        Dropout2d-27             [-1, 16, 7, 7]               0\n",
            "           Conv2d-28             [-1, 16, 5, 5]           2,304\n",
            "             ReLU-29             [-1, 16, 5, 5]               0\n",
            "      BatchNorm2d-30             [-1, 16, 5, 5]              32\n",
            "        Dropout2d-31             [-1, 16, 5, 5]               0\n",
            "           Conv2d-32             [-1, 10, 5, 5]             160\n",
            "        AvgPool2d-33             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 17,200\n",
            "Trainable params: 17,200\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.32\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.39\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-0a0c3e8d4bd7>:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "\n",
        "torch.manual_seed(1) #Fixes the seed to obtain consistent results on every iterations\n",
        "batch_size = 128 #This sets the batch size, majorly 2^x values as GPU stores on 128 or 256 bits at a time\n",
        "\n",
        "# Pin memory is used to reduce data transfer. Ff you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. \n",
        "# This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer.\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} \n",
        "\n",
        "# This loads the training data and performs normalization \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "# This loads the test data and performs normalization \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: [Pin_memory](https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723/2)"
      ],
      "metadata": {
        "id": "J6Cp_7x_wnnm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm# #make your loops show a smart progress meter\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  \n",
        "  \"\"\" Training the model\n",
        "  Args\n",
        "  model: the model which will be trained\n",
        "  device: the device on which model will be trained cpu/gpu\n",
        "  train_loader: the train data loader from torch.utils.data.DataLoader\n",
        "  epoch: the number of epochs for which model runs\n",
        "  optimizer: the optimizer to be used for training\n",
        "  \"\"\"\n",
        "  model.train() # This sets the model on training mode\n",
        "  pbar = tqdm(train_loader)\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    data, target = data.to(device), target.to(device) #This move the data to device\n",
        "    optimizer.zero_grad() #This optimizer zeroes the gradients\n",
        "    output = model(data) #To obtain the output for the data\n",
        "    loss = F.nll_loss(output, target) #loss is negative log likelihood\n",
        "    loss.backward() #This makes the gradients flow backward\n",
        "    optimizer.step() #This performs a parameter update on the current gradient which is stored in .grad attribute of a parameter\n",
        "    pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}') #this is just for beautification of printing\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  \"\"\" Testing the model\n",
        "  Args\n",
        "  model: the model to be tested\n",
        "  device: the device to use for testing\n",
        "  test_loader: the test data loader from torch.utils.data.DataLoader\n",
        "  \"\"\"\n",
        "  model.eval() #This sets the model on eval mode\n",
        "  test_loss = 0 #This sets the test loss to 0\n",
        "  correct = 0 #This signifies number of correct classifications\n",
        "  with torch.no_grad(): #This turns off gradients sicne we are in test mode\n",
        "    \n",
        "    for data, target in test_loader:\n",
        "\n",
        "      data, target = data.to(device), target.to(device)#This moves the data to device\n",
        "      output = model(data) #To obtain the model output\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7807d4de-cc4a-4d16-e450-0416a02431d3"
      },
      "source": [
        "\n",
        "model = Net().to(device) ## move the model to device\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Stochastic gradient descent optimizer with model params, learning rate and momentum\n",
        "\n",
        "num_epoch=20 #defining the epochs\n",
        "for epoch in range(1, num_epoch+1):\n",
        "  print('\\nEpoch {} : '.format(epoch))\n",
        "  train(model, device, train_loader, optimizer, epoch) #Training the model\n",
        "  test(model, device, test_loader) #Testing the model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-2-0a0c3e8d4bd7>:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.24384133517742157 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 23.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0787, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 2 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.11260246485471725 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0421, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 3 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.06747380644083023 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 9889/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 4 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.026839254423975945 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 5 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04351365566253662 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0259, Accuracy: 9919/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 6 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.030737513676285744 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0253, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 7 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.01626867987215519 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 27.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 8 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04324956238269806 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 9930/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 9 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04249363765120506 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 9924/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 10 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.056269872933626175 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0238, Accuracy: 9921/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 11 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.11305997520685196 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0226, Accuracy: 9931/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 12 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.045498643070459366 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 9929/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 13 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.08380056172609329 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 9924/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 14 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.031593725085258484 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 9932/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 15 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.028566718101501465 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 9929/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 16 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.06716521829366684 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 9928/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 17 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.022780269384384155 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0213, Accuracy: 9932/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 18 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.049581289291381836 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 9935/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 19 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.05828255042433739 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 9939/10000 (99%)\n",
            "\n",
            "\n",
            "Epoch 20 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.00918623898178339 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 9938/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The architecture defined above uses 17200 params and achieves validation accuracy of 99.39% in 19th epoch."
      ],
      "metadata": {
        "id": "nJr3_VVO9SxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimentation**\n",
        "\n",
        "The architecutre defined below is an experiment to use less than 5000 params and achieve val accuracy of 99%.\n",
        "\n"
      ],
      "metadata": {
        "id": "9mw49q-MwuZo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(8, 8, 3, bias=False), \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        self.trans1 = nn.Sequential(\n",
        "            nn.Conv2d(8, 8, 1, bias=False), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 8, 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(8, 8, 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 8, 3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        self.avg_pool = nn.Sequential(\n",
        "            nn.Conv2d(8, 10, 1, bias=False),\n",
        "            nn.AvgPool2d(5)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.avg_pool(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available() #checking if CUDA is available\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device) # This transfers the model to the device\n",
        "summary(model, input_size=(1, 28, 28))# This prints the model summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTuKf_6WupM9",
        "outputId": "60ed96ab-b7fd-4fce-f8de-a73b6aace6f8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "              ReLU-2            [-1, 8, 26, 26]               0\n",
            "       BatchNorm2d-3            [-1, 8, 26, 26]              16\n",
            "         Dropout2d-4            [-1, 8, 26, 26]               0\n",
            "            Conv2d-5            [-1, 8, 24, 24]             576\n",
            "              ReLU-6            [-1, 8, 24, 24]               0\n",
            "       BatchNorm2d-7            [-1, 8, 24, 24]              16\n",
            "         Dropout2d-8            [-1, 8, 24, 24]               0\n",
            "            Conv2d-9            [-1, 8, 24, 24]              64\n",
            "             ReLU-10            [-1, 8, 24, 24]               0\n",
            "        MaxPool2d-11            [-1, 8, 12, 12]               0\n",
            "           Conv2d-12            [-1, 8, 10, 10]             576\n",
            "             ReLU-13            [-1, 8, 10, 10]               0\n",
            "      BatchNorm2d-14            [-1, 8, 10, 10]              16\n",
            "        Dropout2d-15            [-1, 8, 10, 10]               0\n",
            "           Conv2d-16              [-1, 8, 8, 8]             576\n",
            "             ReLU-17              [-1, 8, 8, 8]               0\n",
            "      BatchNorm2d-18              [-1, 8, 8, 8]              16\n",
            "        Dropout2d-19              [-1, 8, 8, 8]               0\n",
            "           Conv2d-20              [-1, 8, 8, 8]             576\n",
            "             ReLU-21              [-1, 8, 8, 8]               0\n",
            "      BatchNorm2d-22              [-1, 8, 8, 8]              16\n",
            "        Dropout2d-23              [-1, 8, 8, 8]               0\n",
            "           Conv2d-24             [-1, 10, 8, 8]              80\n",
            "        AvgPool2d-25             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 2,600\n",
            "Trainable params: 2,600\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.45\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.46\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ea8e0a3fe141>:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1) #Fixes the seed to obtain consistent results on every iterations\n",
        "batch_size = 128 #This sets the batch size, majorly 2^x values as GPU stores on 128 or 256 bits at a time\n",
        "\n",
        "# Pin memory is used to reduce data transfer. Ff you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. \n",
        "# This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer.\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} \n",
        "\n",
        "# This loads the training data and performs normalization \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "# This loads the test data and performs normalization \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "metadata": {
        "id": "O8kWv_8Uykhz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm# #make your loops show a smart progress meter\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  \n",
        "  \"\"\" Training the model\n",
        "  Args\n",
        "  model: the model which will be trained\n",
        "  device: the device on which model will be trained cpu/gpu\n",
        "  train_loader: the train data loader from torch.utils.data.DataLoader\n",
        "  epoch: the number of epochs for which model runs\n",
        "  optimizer: the optimizer to be used for training\n",
        "  \"\"\"\n",
        "  model.train() # This sets the model on training mode\n",
        "  pbar = tqdm(train_loader)\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    data, target = data.to(device), target.to(device) #This move the data to device\n",
        "    optimizer.zero_grad() #This optimizer zeroes the gradients\n",
        "    output = model(data) #To obtain the output for the data\n",
        "    loss = F.nll_loss(output, target) #loss is negative log likelihood\n",
        "    loss.backward() #This makes the gradients flow backward\n",
        "    optimizer.step() #This performs a parameter update on the current gradient which is stored in .grad attribute of a parameter\n",
        "    pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}') #this is just for beautification of printing\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  \"\"\" Testing the model\n",
        "  Args\n",
        "  model: the model to be tested\n",
        "  device: the device to use for testing\n",
        "  test_loader: the test data loader from torch.utils.data.DataLoader\n",
        "  \"\"\"\n",
        "  model.eval() #This sets the model on eval mode\n",
        "  test_loss = 0 #This sets the test loss to 0\n",
        "  correct = 0 #This signifies number of correct classifications\n",
        "  with torch.no_grad(): #This turns off gradients sicne we are in test mode\n",
        "    \n",
        "    for data, target in test_loader:\n",
        "\n",
        "      data, target = data.to(device), target.to(device)#This moves the data to device\n",
        "      output = model(data) #To obtain the model output\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "yvzxCIdWyqo1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device) ## move the model to device\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Stochastic gradient descent optimizer with model params, learning rate and momentum\n",
        "\n",
        "num_epoch=20 #defining the epochs\n",
        "for epoch in range(1, num_epoch+1):\n",
        "  print('\\nEpoch {} : '.format(epoch))\n",
        "  train(model, device, train_loader, optimizer, epoch) #Training the model\n",
        "  test(model, device, test_loader) #Testing the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aLV3Du1y1eG",
        "outputId": "bf2a84c1-f2fd-4e8c-914c-5e67d6f43cb3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-14-ea8e0a3fe141>:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.47719481587409973 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.2480, Accuracy: 9318/10000 (93%)\n",
            "\n",
            "\n",
            "Epoch 2 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.3515276610851288 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1351, Accuracy: 9588/10000 (96%)\n",
            "\n",
            "\n",
            "Epoch 3 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.22162796556949615 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1236, Accuracy: 9646/10000 (96%)\n",
            "\n",
            "\n",
            "Epoch 4 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.2028311938047409 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0990, Accuracy: 9699/10000 (97%)\n",
            "\n",
            "\n",
            "Epoch 5 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.21560150384902954 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0890, Accuracy: 9712/10000 (97%)\n",
            "\n",
            "\n",
            "Epoch 6 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.24940691888332367 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0841, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "\n",
            "Epoch 7 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.26770034432411194 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0769, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 8 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.1762668639421463 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0765, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 9 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.34397467970848083 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0757, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 10 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.17334377765655518 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0741, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 11 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.11125566810369492 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0688, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 12 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.1741122156381607 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0734, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 13 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.11284058541059494 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0657, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 14 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.135837122797966 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0678, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 15 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.10651921480894089 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0673, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 16 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.2212749570608139 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0688, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 17 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.17073588073253632 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0706, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 18 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.24766004085540771 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0655, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 19 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.15622377395629883 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0642, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "\n",
            "Epoch 20 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.18333594501018524 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0625, Accuracy: 9802/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The architecture defined for experimentation uses 2600 params and achieve an accuracy of 98.06% in 19th epoch.**"
      ],
      "metadata": {
        "id": "Xq08nnXP9fp2"
      }
    }
  ]
}