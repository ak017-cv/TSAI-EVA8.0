{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/141T6VxqOB4hkjcwHqbltU8LzwwHpOtlG\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "iDaWIw_GjFOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Drill 02"
      ],
      "metadata": {
        "id": "-FUi2wYjFwub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Target\n",
        "\n",
        "- Reduce the number of parameters\n",
        "- Add Transformations\n",
        "- By close examination of the dataset we see that `-7deg to 7deg` rotations should work for us.\n",
        "- Add another layer after the GAP, possibly to capture more features.\n",
        "\n",
        "## 2. Result\n",
        "\n",
        "- Params: `9,962`\n",
        "- Best Train Accuracy(Epoch 14): `98.35%`\n",
        "- Best Test Accuracy(Epoch 15) : `99.35%`\n",
        "\n",
        "## 3. Analysis\n",
        "\n",
        "- We added transformations/augmentation to compensate the removal of params, now we need to train the remaining neurons harder\n",
        "- There was a slight increase in the validation accuracy now\n",
        "- We've reached our target parameters, now we'll try to reach our target accuracy, but we couldn't.\n",
        "- There are a lot of oscillations in loss and accuracy, we could try to reduce this by using a LR Scheduler in the next iteration"
      ],
      "metadata": {
        "id": "oqf0je2tF4lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "DJtqhnxyF-sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "mPqvW5JTF5yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transfomrations"
      ],
      "metadata": {
        "id": "XRTGvOLsGIVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "metadata": {
        "id": "2CZT9pv2GDMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Train/Test Split"
      ],
      "metadata": {
        "id": "l3O_AgvxGfwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "metadata": {
        "id": "ZbVh6UvBGKgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader Arguments & Test/Train Dataloaders"
      ],
      "metadata": {
        "id": "Uot6-TG7GgAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# note about pin_memory\n",
        "# If you load your samples in the Dataset on CPU and would like to push it\n",
        "# during training to the GPU, you can speed up the host to device transfer by\n",
        "# enabling pin_memory. This lets your DataLoader allocate the samples in\n",
        "# page-locked memory, which speeds-up the transfer.\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLM6GM5IGq0k",
        "outputId": "dc6890c6-d966-44bd-e7e1-5377c6a4c4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-CDC8yiNG6D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "dropout_value = 0.1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=14, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(14),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 26\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=14, out_channels=30, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(30),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 24\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=30, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 24\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(14),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 10\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=14, out_channels=15, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(15),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 8\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=15, out_channels=15, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(15),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 6\n",
        "        \n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=6)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=15, out_channels=15, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(15),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=15, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.gap(x)        \n",
        "        x = self.convblock7(x)\n",
        "        x = self.convblock8(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "n5CKIrVCG7mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Params"
      ],
      "metadata": {
        "id": "4tX3KR4wHICs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN8zpkwoHKwg",
        "outputId": "1532ac78-a4c6-485d-a071-1d558a46e515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 14, 26, 26]             126\n",
            "              ReLU-2           [-1, 14, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 14, 26, 26]              28\n",
            "           Dropout-4           [-1, 14, 26, 26]               0\n",
            "            Conv2d-5           [-1, 30, 24, 24]           3,780\n",
            "              ReLU-6           [-1, 30, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 30, 24, 24]              60\n",
            "           Dropout-8           [-1, 30, 24, 24]               0\n",
            "            Conv2d-9           [-1, 10, 24, 24]             300\n",
            "        MaxPool2d-10           [-1, 10, 12, 12]               0\n",
            "           Conv2d-11           [-1, 14, 10, 10]           1,260\n",
            "             ReLU-12           [-1, 14, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 14, 10, 10]              28\n",
            "          Dropout-14           [-1, 14, 10, 10]               0\n",
            "           Conv2d-15             [-1, 15, 8, 8]           1,890\n",
            "             ReLU-16             [-1, 15, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 15, 8, 8]              30\n",
            "          Dropout-18             [-1, 15, 8, 8]               0\n",
            "           Conv2d-19             [-1, 15, 6, 6]           2,025\n",
            "             ReLU-20             [-1, 15, 6, 6]               0\n",
            "      BatchNorm2d-21             [-1, 15, 6, 6]              30\n",
            "          Dropout-22             [-1, 15, 6, 6]               0\n",
            "        AvgPool2d-23             [-1, 15, 1, 1]               0\n",
            "           Conv2d-24             [-1, 15, 1, 1]             225\n",
            "      BatchNorm2d-25             [-1, 15, 1, 1]              30\n",
            "             ReLU-26             [-1, 15, 1, 1]               0\n",
            "          Dropout-27             [-1, 15, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             150\n",
            "================================================================\n",
            "Total params: 9,962\n",
            "Trainable params: 9,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.96\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 1.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing"
      ],
      "metadata": {
        "id": "fweOOcVfHXdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this automatically selects tqdm for colab_notebook\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)#, ncols=\"80%\")\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do \n",
        "    # backpropragation because PyTorch accumulates the gradients on subsequent \n",
        "    # backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should \n",
        "    # zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # get the index of the max log-probability\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "02LjH5s3HPHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device) ## move the model to device\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Stochastic gradient descent optimizer with model params, learning rate and momentum\n",
        "\n",
        "num_epoch=15 #defining the epochs\n",
        "for epoch in range(1, num_epoch+1):\n",
        "  print('\\nEpoch {} : '.format(epoch))\n",
        "  train(model, device, train_loader, optimizer, epoch) #Training the model\n",
        "  test(model, device, test_loader) #Testing the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzP_spReHaPZ",
        "outputId": "4ab1354a-9ac9-4532-8ed2-daaaf03534df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18490050733089447 Batch_id=468 Accuracy=86.74: 100%|██████████| 469/469 [00:18<00:00, 25.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0638, Accuracy: 9833/10000 (98.33%)\n",
            "\n",
            "\n",
            "Epoch 2 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09858846664428711 Batch_id=468 Accuracy=95.86: 100%|██████████| 469/469 [00:17<00:00, 26.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0464, Accuracy: 9867/10000 (98.67%)\n",
            "\n",
            "\n",
            "Epoch 3 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.036158159375190735 Batch_id=468 Accuracy=96.70: 100%|██████████| 469/469 [00:18<00:00, 25.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0368, Accuracy: 9892/10000 (98.92%)\n",
            "\n",
            "\n",
            "Epoch 4 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05300205573439598 Batch_id=468 Accuracy=97.12: 100%|██████████| 469/469 [00:18<00:00, 26.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0336, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "\n",
            "Epoch 5 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1249152198433876 Batch_id=468 Accuracy=97.37: 100%|██████████| 469/469 [00:19<00:00, 24.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "\n",
            "Epoch 6 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.21155650913715363 Batch_id=468 Accuracy=97.64: 100%|██████████| 469/469 [00:18<00:00, 25.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0318, Accuracy: 9903/10000 (99.03%)\n",
            "\n",
            "\n",
            "Epoch 7 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16327281296253204 Batch_id=468 Accuracy=97.83: 100%|██████████| 469/469 [00:17<00:00, 26.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9909/10000 (99.09%)\n",
            "\n",
            "\n",
            "Epoch 8 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04933523014187813 Batch_id=468 Accuracy=97.85: 100%|██████████| 469/469 [00:17<00:00, 26.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0253, Accuracy: 9927/10000 (99.27%)\n",
            "\n",
            "\n",
            "Epoch 9 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14054745435714722 Batch_id=468 Accuracy=97.93: 100%|██████████| 469/469 [00:17<00:00, 26.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0247, Accuracy: 9925/10000 (99.25%)\n",
            "\n",
            "\n",
            "Epoch 10 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04613813757896423 Batch_id=468 Accuracy=98.07: 100%|██████████| 469/469 [00:17<00:00, 26.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "\n",
            "Epoch 11 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.014683539979159832 Batch_id=468 Accuracy=98.13: 100%|██████████| 469/469 [00:17<00:00, 26.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "\n",
            "Epoch 12 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05391973257064819 Batch_id=468 Accuracy=98.26: 100%|██████████| 469/469 [00:17<00:00, 26.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0260, Accuracy: 9928/10000 (99.28%)\n",
            "\n",
            "\n",
            "Epoch 13 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04033292084932327 Batch_id=468 Accuracy=98.19: 100%|██████████| 469/469 [00:17<00:00, 26.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0237, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "\n",
            "Epoch 14 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.01822119764983654 Batch_id=468 Accuracy=98.35: 100%|██████████| 469/469 [00:18<00:00, 24.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0288, Accuracy: 9917/10000 (99.17%)\n",
            "\n",
            "\n",
            "Epoch 15 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.016930770128965378 Batch_id=468 Accuracy=98.30: 100%|██████████| 469/469 [00:17<00:00, 26.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0226, Accuracy: 9935/10000 (99.35%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}